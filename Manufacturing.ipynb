{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic stats about the size of files\n",
    "# They all have 69742 rows\n",
    "# They all have the same number of columns per row as the number of columns in the header\n",
    "# Numbers of columns are 2142 for train_cat.csv, 1158 for train_date.csv, 970 for train_numeric.csv.\n",
    "\n",
    "def get_basic_stats(filename):\n",
    "    count = 0\n",
    "    with open('bosch_small_data/{}'.format(filename)) as fp:\n",
    "        line = fp.readline()\n",
    "        num_columns = len(line.split(','))\n",
    "        row_lengths = []\n",
    "        while line:\n",
    "            line = fp.readline()\n",
    "            if line:\n",
    "                count = count+1\n",
    "                row_length = len(line.split(','))\n",
    "                if (row_length != num_columns):\n",
    "                    print(count)\n",
    "                row_lengths.append(row_length)\n",
    "    return [count, num_columns, np.unique(row_lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other stuff that has been verified\n",
    "\n",
    "# All parts are ordered by ID in each training data set, but not in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79926\n",
      "162193\n",
      "289593\n",
      "339074\n",
      "386604\n",
      "387364\n",
      "430156\n",
      "475302\n",
      "578159\n",
      "598386\n",
      "602480\n",
      "605358\n",
      "632286\n",
      "635586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7fb2138879ee>\u001b[0m in \u001b[0;36mis_number\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-cd3910138d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mheader_stem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Of the form L[X]S[Y], which is now stations will be identified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;31m# Make sure we get the minimum valid date.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mheader_stem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheader_stem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7fb2138879ee>\u001b[0m in \u001b[0;36mis_number\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get a count of station tranfers throughout the data set.\n",
    "# Following, assume the order of stations for a given part is based on the order of the marking in the date field.\n",
    "# When there are different times for measurements at a station, the actual time is assumed to be the first one.\n",
    "\n",
    "# An object indicating the number of parts that go from station X to Y for each X and Y.\n",
    "station_map = {}\n",
    "# An object of the form {LxSy: {LzSw:[1,2,3,...]}}, indicating parts for which one station is succssor of another.\n",
    "successors = {}\n",
    "# List of all stations by part ID\n",
    "station_lists = {}\n",
    "\n",
    "with open('bosch_small_data/train_date.csv') as fp:\n",
    "    line = fp.readline() # The line as a single text string\n",
    "    header = line.split(',') # Array of each of the columns\n",
    "    line_count = 0 # Count the total number of lines read, not including the header line.\n",
    "    while(line): # while(line) to read through them all\n",
    "        line = fp.readline().strip() # Single text string\n",
    "        dates = line.split(',') # Get all values. Most are dates, except first is the ID and last is response.\n",
    "        line_count += 1\n",
    "        d = {} # An object containing all stations that the part visits and the (earliest) time of the visit.\n",
    "        #if (line_count % 1000 == 0):\n",
    "        #    print(line_count)\n",
    "        if line:\n",
    "            for i in range(1,len(header)-1):\n",
    "                blocks = header[i].split('_')\n",
    "                header_stem = blocks[0]+blocks[1] # Of the form L[X]S[Y], which is now stations will be identified.\n",
    "                # Make sure we get the minimum valid date.\n",
    "                if is_number(dates[i]):\n",
    "                    if header_stem not in d:\n",
    "                        d[header_stem] = float(dates[i])\n",
    "                    else:\n",
    "                        d[header_stem] = min(float(dates[i]), d[header_stem])\n",
    "            stations = list(d.keys()) # The same values as header_stem, as above.\n",
    "            sorted(stations, key=lambda l:d[l]) # Sort by date. In case of tie, keep initial order.\n",
    "            \n",
    "            # Convert stations list to the map for the Sankey diagram\n",
    "            if (len(stations) > 0):\n",
    "                # Start stations\n",
    "                if (\"Start \"+stations[0]) not in station_map:\n",
    "                    station_map[\"Start \"+stations[0]] = 1\n",
    "                else:\n",
    "                    station_map[\"Start \"+stations[0]] += 1\n",
    "                if \"Start\" not in successors:\n",
    "                    successors[\"Start\"] = {}\n",
    "                if stations[0] not in successors[\"Start\"]:\n",
    "                    successors[\"Start\"][stations[0]] = []\n",
    "                successors[\"Start\"][stations[0]].append(dates[0]) # Append ID\n",
    "                    \n",
    "                # Intermediate stations\n",
    "                for i in range(len(stations)-1):\n",
    "                    if (stations[i]+\" \"+stations[i+1]) not in station_map:\n",
    "                        station_map[stations[i]+\" \"+stations[i+1]] = 1\n",
    "                    else:\n",
    "                        station_map[stations[i]+\" \"+stations[i+1]] += 1\n",
    "                    if stations[i] not in successors:\n",
    "                        successors[stations[i]] = {}\n",
    "                    if stations[i+1] not in successors[stations[i]]:\n",
    "                        successors[stations[i]][stations[i+1]] = []\n",
    "                    successors[stations[i]][stations[i+1]].append(dates[0]) # Append ID\n",
    "                    \n",
    "                # Final station, which 'Good' or 'Bad' endpoint based on response.\n",
    "                end_mapper = {\"0.0\":\"Good\", \"1.0\":\"Bad\"}[dates[-1]]\n",
    "                if (stations[-1]+\" \"+end_mapper) not in station_map:\n",
    "                    station_map[stations[-1]+\" \"+end_mapper] = 1\n",
    "                else:\n",
    "                    station_map[stations[-1]+\" \"+end_mapper] += 1\n",
    "                if stations[-1] not in successors:\n",
    "                    successors[stations[-1]] = {}\n",
    "                if end_mapper not in successors[stations[-1]]:\n",
    "                    successors[stations[-1]][end_mapper] = []\n",
    "                successors[stations[-1]][end_mapper].append(dates[0]) # Append ID\n",
    "                \n",
    "                # Store the full list of stations\n",
    "                station_lists[dates[0]] = [\"Start\"]+stations+[end_mapper]\n",
    "\n",
    "                    \n",
    "filehandler = open(\"station_map.obj\", 'wb')\n",
    "pickle.dump(station_map, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open(\"successors.obj\", 'wb')\n",
    "pickle.dump(successors, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open(\"station_lists.obj\", 'wb')\n",
    "pickle.dump(station_lists, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this to load the station map from file rather than recreate it\n",
    "with open(\"station_map.obj\", \"rb\") as input_file:\n",
    "    station_map = pickle.load(input_file)\n",
    "with open(\"successors.obj\", \"rb\") as input_file:\n",
    "    successors = pickle.load(input_file)\n",
    "with open(\"station_lists.obj\", \"rb\") as input_file:\n",
    "    station_lists = pickle.load(input_file)\n",
    "# From the list of successors, just get the number of successors from one station to another.\n",
    "# Same data as in station_map, but organized more explicity by starting station.\n",
    "successor_lengths = {}\n",
    "for key in successors:\n",
    "    successor_lengths[key] = {}\n",
    "    for key2 in successors[key]:\n",
    "        successor_lengths[key][key2] = len(successors[key][key2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert station map to a destination dictionary for each station\n",
    "transfer_dict = {}\n",
    "for key in station_map:\n",
    "    [start, end] = key.split(\" \")\n",
    "    if start not in transfer_dict:\n",
    "        transfer_dict[start] = {}\n",
    "    transfer_dict[start][end] = station_map[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L0S17': 3656, 'L0S16': 3511, 'L0S15': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successor_lengths[\"L0S14\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(station):\n",
    "    station_blocks = station.split('S')\n",
    "    station_alt = station_blocks[0]+\"_S\"+station_blocks[1]\n",
    "    with open('bosch_small_data/train_date.csv') as fp_date:\n",
    "        with open('bosch_small_data/train_cat.csv') as fp_cat:\n",
    "            with open('bosch_small_data/train_numeric.csv') as fp_numeric:\n",
    "\n",
    "                # Get the column numbers of interest\n",
    "                date_headers = fp_date.readline().strip().split(',')\n",
    "                cat_headers = fp_cat.readline().strip().split(',')\n",
    "                numeric_headers = fp_numeric.readline().strip().split(',')\n",
    "                date_columns = []\n",
    "                cat_columns = []\n",
    "                numeric_columns = []\n",
    "                for i in range(len(date_headers)):\n",
    "                    if station_alt in date_headers[i]:\n",
    "                        date_columns.append(i)\n",
    "                for i in range(len(cat_headers)):\n",
    "                    if station_alt in cat_headers[i]:\n",
    "                        cat_columns.append(i)\n",
    "                for i in range(len(numeric_headers)):\n",
    "                    if station_alt in numeric_headers[i]:\n",
    "                        numeric_columns.append(i)\n",
    "\n",
    "                # Set up data object\n",
    "                # station_cat, station_numeric, and station_response will be the columns for the given station.\n",
    "                station_cat = {}\n",
    "                station_numeric = {}\n",
    "                station_response = []\n",
    "                for i in range(len(cat_columns)):\n",
    "                    station_cat[cat_headers[cat_columns[i]]] = []\n",
    "                for i in range(len(numeric_columns)):\n",
    "                    station_numeric[numeric_headers[numeric_columns[i]]] = []\n",
    "\n",
    "                # Read line by line and fill in data\n",
    "                line_date = 'asdf'\n",
    "                line_cat = \"\"\n",
    "                line_numeric = \"\"\n",
    "                while line_date:\n",
    "                    line_date = fp_date.readline()\n",
    "                    line_cat = fp_cat.readline()\n",
    "                    line_numeric = fp_numeric.readline()\n",
    "                    if line_date:\n",
    "                        line_date_ = line_date.strip().split(',')\n",
    "                        line_cat_ = line_cat.strip().split(',')\n",
    "                        line_numeric_ = line_numeric.strip().split(',')\n",
    "                        id_ = line_date_[0]\n",
    "                        if id_ in station_lists and station in station_lists[id_]:\n",
    "                            for i in range(len(cat_columns)):\n",
    "                                station_cat[cat_headers[cat_columns[i]]].append(line_cat_[cat_columns[i]])\n",
    "                            for i in range(len(numeric_columns)):\n",
    "                                val = line_numeric_[numeric_columns[i]]\n",
    "                                if not is_number(val):\n",
    "                                    val = '0'\n",
    "                                station_numeric[numeric_headers[numeric_columns[i]]].append(val)\n",
    "                            station_response.append(station_lists[id_][station_lists[id_].index(station)+1])\n",
    "                return {\"cat\":station_cat, \"numeric\":station_numeric, \"response\":station_response}\n",
    "#training_data = get_training_data(\"L0S14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next thing to do\n",
    "# Use station_cat, station_numeric to predict station_response (maybe just station_numeric)\n",
    "\n",
    "# Response in a form appropriate for scikit-learn.\n",
    "def convert(x,s):\n",
    "    if x == s:\n",
    "        return 1\n",
    "    return 0\n",
    "def get_scores(station, training_data):\n",
    "    successors_ = np.unique(training_data[\"response\"])\n",
    "    X = [training_data[\"numeric\"][list(training_data[\"numeric\"].keys())[i]] for i in range(len(training_data[\"numeric\"].keys()))]\n",
    "    X = np.array(X).transpose().astype(float)\n",
    "    scores = {}\n",
    "    for i in range(len(successors_)):\n",
    "        if successor_lengths[station][successors_[i]] > 100:\n",
    "            y = np.array(list(map(lambda x:convert(x,successors_[i]), training_data[\"response\"])))\n",
    "            reg = LinearRegression().fit(X, y)\n",
    "            scores[successors_[i]] = reg.score(X,y)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L0S13\n",
      "L0S14\n",
      "L0S17\n",
      "L0S20\n",
      "L0S21\n",
      "L3S30\n",
      "L3S34\n",
      "L3S37\n",
      "L0S1\n",
      "L0S3\n",
      "L0S4\n",
      "L0S8\n",
      "L0S11\n",
      "L0S22\n",
      "L0S5\n",
      "L0S9\n",
      "L0S16\n",
      "L0S23\n",
      "L2S27\n",
      "L0S15\n",
      "L1S24\n",
      "L0S2\n",
      "L0S10\n",
      "L2S26\n",
      "L3S41\n",
      "L3S48\n",
      "L3S51\n",
      "L3S32\n",
      "L1S25\n",
      "L3S38\n"
     ]
    }
   ],
   "source": [
    "# Now do it for all stations\n",
    "overall_results = {}\n",
    "# Some bad stations have successors with missing numeric data. For now, just skip them\n",
    "#bad_stations = [\"Start\",\"L0S21\",\"L3S30\",\"L0S1\"]\n",
    "bad_stations=[\"Start\"]\n",
    "for station in successor_lengths:\n",
    "    if station not in bad_stations:\n",
    "        num = 0 # Number of successors with over 100 parts. Need at least two of them\n",
    "        for key in successor_lengths[station]:\n",
    "            if successor_lengths[station][key] > 100:\n",
    "                num += 1\n",
    "        if num>1:\n",
    "            print(station)\n",
    "            training_data = get_training_data(station)\n",
    "            if len(training_data[\"numeric\"]) > 0:\n",
    "                overall_results[station] = get_scores(station, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall_results # Scores, which are R^2 values.\n",
    "# L3S30, L1S24, L1S25 are, on a brief scan, the only ones with any successors with an R^2 > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L2S26': 2980,\n",
       " 'L3S38': 3,\n",
       " 'L2S27': 1346,\n",
       " 'L3S29': 474,\n",
       " 'L2S28': 117,\n",
       " 'L3S30': 3,\n",
       " 'L3S39': 49}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "successor_lengths[\"L1S25\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presentation\n",
    "# 1) Introduce the data set\n",
    "# 2) Show the flow diagram of parts. Consider https://www.kaggle.com/gingerman/shopfloor-visualization\n",
    "# 3) General question, do measurements influence the path that a part takes through the line?\n",
    "# 4) Hypothesis: a linear model can predict the path\n",
    "# 5) Describe experimental setup.\n",
    "# 6) Result: no evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
